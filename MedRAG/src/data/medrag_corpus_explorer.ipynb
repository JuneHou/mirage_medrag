{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e6b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09dd568",
   "metadata": {},
   "source": [
    "# MedRAG Corpus Statistics\n",
    "\n",
    "This notebook computes corpus statistics for the 4 MedRAG sources: PubMed, Wikipedia, Textbooks, and StatPearls.\n",
    "\n",
    "**Measurements:**\n",
    "- **#Doc.**: Number of unique documents \n",
    "- **#Snippets**: Number of text chunks\n",
    "- **Avg. L**: Average snippet length in characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e06ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "BASE_CORPUS_DIR = Path('/data/wang/junh/githubs/MedRAG/src/data/corpus')\n",
    "HF_CACHE_DIR = Path('/data/wang/junh/githubs/MedRAG/src/data/hf_cache')\n",
    "\n",
    "print('Corpus directory:', BASE_CORPUS_DIR)\n",
    "print('HF cache directory:', HF_CACHE_DIR)\n",
    "print('Corpus exists:', BASE_CORPUS_DIR.exists())\n",
    "print('HF cache exists:', HF_CACHE_DIR.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d389636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_content(obj):\n",
    "    \"\"\"Extract text content from a JSON object.\"\"\"\n",
    "    for field in ['content', 'contents', 'text', 'body']:\n",
    "        if field in obj and isinstance(obj[field], str):\n",
    "            return obj[field]\n",
    "    return \"\"\n",
    "\n",
    "def get_document_id(obj):\n",
    "    \"\"\"Extract document ID from a JSON object.\"\"\"\n",
    "    for field in ['PMID', 'pmid', 'paper_id', 'paperId', 'document_id', 'doc_id']:\n",
    "        if field in obj:\n",
    "            return str(obj[field])\n",
    "    return None\n",
    "\n",
    "def process_corpus(corpus_path):\n",
    "    \"\"\"Process a single corpus and return statistics.\"\"\"\n",
    "    corpus_name = corpus_path.name\n",
    "    chunk_dir = corpus_path / 'chunk'\n",
    "    \n",
    "    if not chunk_dir.exists():\n",
    "        print(f\"No chunk directory found for {corpus_name}\")\n",
    "        return None\n",
    "    \n",
    "    chunk_files = list(chunk_dir.glob('*.jsonl'))\n",
    "    if not chunk_files:\n",
    "        print(f\"No JSONL files found for {corpus_name}\")\n",
    "        return None\n",
    "    \n",
    "    total_snippets = 0\n",
    "    total_chars = 0\n",
    "    unique_docs = set()\n",
    "    \n",
    "    print(f\"Processing {corpus_name} ({len(chunk_files)} files)...\")\n",
    "    \n",
    "    for file_path in tqdm(chunk_files, desc=f\"{corpus_name}\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        total_snippets += 1\n",
    "                        \n",
    "                        # Get text content and length\n",
    "                        text = get_text_content(obj)\n",
    "                        total_chars += len(text)\n",
    "                        \n",
    "                        # Get document ID\n",
    "                        doc_id = get_document_id(obj)\n",
    "                        if doc_id:\n",
    "                            unique_docs.add(doc_id)\n",
    "                            \n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_length = total_chars / total_snippets if total_snippets > 0 else 0\n",
    "    n_docs = len(unique_docs) if unique_docs else total_snippets\n",
    "    \n",
    "    return {\n",
    "        'Corpus': corpus_name,\n",
    "        '#Doc.': n_docs,\n",
    "        '#Snippets': total_snippets,\n",
    "        'Avg. L': round(avg_length)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de3e143f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pubmed (1166 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pubmed:   0%|          | 0/1166 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pubmed: 100%|██████████| 1166/1166 [03:51<00:00,  5.04it/s]\n",
      "pubmed: 100%|██████████| 1166/1166 [03:51<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wikipedia (646 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wikipedia: 100%|██████████| 646/646 [02:53<00:00,  3.72it/s]\n",
      "wikipedia: 100%|██████████| 646/646 [02:53<00:00,  3.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing textbooks (18 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textbooks: 100%|██████████| 18/18 [00:00<00:00, 22.70it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing statpearls (9625 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "statpearls: 100%|██████████| 9625/9625 [00:05<00:00, 1848.76it/s]\n",
      "statpearls: 100%|██████████| 9625/9625 [00:05<00:00, 1848.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MedRAG Corpus Statistics:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>#Doc. (M)</th>\n",
       "      <th>#Snippets (M)</th>\n",
       "      <th>Avg. L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pubmed</td>\n",
       "      <td>23.9</td>\n",
       "      <td>23.9</td>\n",
       "      <td>1309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>29.9</td>\n",
       "      <td>29.9</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>textbooks</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>statpearls</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MedCorp (Total)</td>\n",
       "      <td>54.3</td>\n",
       "      <td>54.3</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Corpus  #Doc. (M)  #Snippets (M) Avg. L\n",
       "0           pubmed       23.9           23.9   1309\n",
       "1        wikipedia       29.9           29.9    682\n",
       "2        textbooks        0.1            0.1    777\n",
       "3       statpearls        0.4            0.4    516\n",
       "4  MedCorp (Total)       54.3           54.3      -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Numbers:\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>#Doc.</th>\n",
       "      <th>#Snippets</th>\n",
       "      <th>Avg. L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pubmed</td>\n",
       "      <td>23895135</td>\n",
       "      <td>23898701</td>\n",
       "      <td>1309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>29913202</td>\n",
       "      <td>29913202</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>textbooks</td>\n",
       "      <td>125847</td>\n",
       "      <td>125847</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>statpearls</td>\n",
       "      <td>352155</td>\n",
       "      <td>352155</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MedCorp (Total)</td>\n",
       "      <td>54289905</td>\n",
       "      <td>54289905</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Corpus     #Doc.  #Snippets Avg. L\n",
       "0           pubmed  23895135   23898701   1309\n",
       "1        wikipedia  29913202   29913202    682\n",
       "2        textbooks    125847     125847    777\n",
       "3       statpearls    352155     352155    516\n",
       "4  MedCorp (Total)  54289905   54289905      -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find available corpora\n",
    "corpus_names = ['pubmed', 'wikipedia', 'textbooks', 'statpearls']\n",
    "results = []\n",
    "\n",
    "for corpus_name in corpus_names:\n",
    "    corpus_path = BASE_CORPUS_DIR / corpus_name\n",
    "    if corpus_path.exists():\n",
    "        stats = process_corpus(corpus_path)\n",
    "        if stats:\n",
    "            results.append(stats)\n",
    "    else:\n",
    "        print(f\"Corpus not found: {corpus_path}\")\n",
    "\n",
    "# Create DataFrame\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Format numbers for better display\n",
    "    df['#Doc. (M)'] = (df['#Doc.'] / 1_000_000).round(1)\n",
    "    df['#Snippets (M)'] = (df['#Snippets'] / 1_000_000).round(1)\n",
    "    \n",
    "    # Add MedCorp total if available\n",
    "    if 'medcorp_count' in globals() and medcorp_count:\n",
    "        medcorp_row = {\n",
    "            'Corpus': 'MedCorp (Total)',\n",
    "            '#Doc.': medcorp_count,\n",
    "            '#Snippets': medcorp_count,\n",
    "            'Avg. L': '-',\n",
    "            '#Doc. (M)': round(medcorp_count / 1_000_000, 1),\n",
    "            '#Snippets (M)': round(medcorp_count / 1_000_000, 1)\n",
    "        }\n",
    "        df = pd.concat([df, pd.DataFrame([medcorp_row])], ignore_index=True)\n",
    "    \n",
    "    # Display the main table\n",
    "    display_df = df[['Corpus', '#Doc. (M)', '#Snippets (M)', 'Avg. L']].copy()\n",
    "    print(\"\\nMedRAG Corpus Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    display(display_df)\n",
    "    \n",
    "    # Also show raw numbers\n",
    "    print(\"\\nRaw Numbers:\")\n",
    "    print(\"=\" * 30)\n",
    "    display(df[['Corpus', '#Doc.', '#Snippets', 'Avg. L']])\n",
    "else:\n",
    "    print(\"No corpus data found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf268d8d",
   "metadata": {},
   "source": [
    "## Check MedCorp Mapping File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a799363b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting entries in MedCorp_id2text.json...\n",
      "File size: 57.1 GB\n",
      "\n",
      "Reading example entry...\n",
      "Example entry:\n",
      "  ID: {\"pubmed23n0001_0\n",
      "  Title: [Biochemical studies on camomile components/III. In vitro studies about the antipeptic activity of (...\n",
      "  Content length: 395 chars\n",
      "  PMID: 21\n",
      "Raw format example: {\"pubmed23n0001_0\": {\"id\": \"pubmed23n0001_0\", \"title\": \"[Biochemical studies on camomile components/III. In vitro studies about the antipeptic activity of (--)-alpha-bisabolol (author's transl)].\", \"c...\n",
      "\n",
      "Counting all entries...\n",
      "\n",
      "MedCorp mapping file contains: 54,289,905 entries\n",
      "MedCorp entries (millions): 54.3M\n",
      "\n",
      "MedCorp mapping file contains: 54,289,905 entries\n",
      "MedCorp entries (millions): 54.3M\n"
     ]
    }
   ],
   "source": [
    "def count_medcorp_entries():\n",
    "    \"\"\"Count entries in MedCorp_id2text.json without loading the entire file.\"\"\"\n",
    "    medcorp_file = BASE_CORPUS_DIR / 'MedCorp_id2text.json'\n",
    "    \n",
    "    if not medcorp_file.exists():\n",
    "        print(f\"MedCorp file not found: {medcorp_file}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Counting entries in {medcorp_file.name}...\")\n",
    "    print(f\"File size: {medcorp_file.stat().st_size / (1024*1024*1024):.1f} GB\")\n",
    "    \n",
    "    # First, show an example entry by reading the first few lines\n",
    "    print(\"\\nReading example entry...\")\n",
    "    try:\n",
    "        with open(medcorp_file, 'r', encoding='utf-8') as f:\n",
    "            # Read first few KB to find a complete entry\n",
    "            sample_text = f.read(10000)  # Read 10KB\n",
    "            \n",
    "            # Parse the format: \"id\": {\"nested\": \"json\"}\n",
    "            if '\"' in sample_text and ':' in sample_text:\n",
    "                # Find first complete entry\n",
    "                lines = sample_text.split('\\n')\n",
    "                for line in lines[:10]:  # Check first 10 lines\n",
    "                    line = line.strip().rstrip(',')  # Remove trailing comma\n",
    "                    if '\"' in line and ':' in line and '{' in line:\n",
    "                        # Try to extract ID and object\n",
    "                        try:\n",
    "                            # Look for pattern: \"id\": {object}\n",
    "                            if '\": {' in line:\n",
    "                                id_part = line.split('\": {')[0].strip('\"').strip()\n",
    "                                # Find the matching closing brace for this entry\n",
    "                                brace_count = 0\n",
    "                                start_idx = line.find('\": {') + 3\n",
    "                                obj_start = start_idx\n",
    "                                for i, char in enumerate(line[start_idx:], start_idx):\n",
    "                                    if char == '{':\n",
    "                                        brace_count += 1\n",
    "                                    elif char == '}':\n",
    "                                        brace_count -= 1\n",
    "                                        if brace_count == 0:\n",
    "                                            # Found complete object\n",
    "                                            obj_text = line[obj_start:i+1]\n",
    "                                            try:\n",
    "                                                obj = json.loads(obj_text)\n",
    "                                                print(f\"Example entry:\")\n",
    "                                                print(f\"  ID: {id_part}\")\n",
    "                                                print(f\"  Title: {obj.get('title', 'N/A')[:100]}...\")\n",
    "                                                print(f\"  Content length: {len(obj.get('content', ''))} chars\")\n",
    "                                                print(f\"  PMID: {obj.get('PMID', 'N/A')}\")\n",
    "                                                break\n",
    "                                            except:\n",
    "                                                continue\n",
    "                        except:\n",
    "                            continue\n",
    "                    if 'Example entry:' in locals():\n",
    "                        break\n",
    "                        \n",
    "                # Fallback: show raw format\n",
    "                if 'Example entry:' not in str(locals()):\n",
    "                    first_line = sample_text.split('\\n')[0][:200]\n",
    "                    print(f\"Raw format example: {first_line}...\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Could not read example entry: {e}\")\n",
    "    \n",
    "    print(\"\\nCounting all entries...\")\n",
    "    count = 0\n",
    "    buffer_size = 1024 * 1024  # 1MB buffer for faster reading\n",
    "    \n",
    "    try:\n",
    "        with open(medcorp_file, 'r', encoding='utf-8') as f:\n",
    "            # Read in chunks for better performance\n",
    "            in_string = False\n",
    "            escape_next = False\n",
    "            found_first_brace = False\n",
    "            brace_depth = 0\n",
    "            \n",
    "            while True:\n",
    "                chunk = f.read(buffer_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                \n",
    "                for char in chunk:\n",
    "                    if not found_first_brace:\n",
    "                        if char == '{':\n",
    "                            found_first_brace = True\n",
    "                            brace_depth = 1\n",
    "                        continue\n",
    "                    \n",
    "                    if escape_next:\n",
    "                        escape_next = False\n",
    "                        continue\n",
    "                        \n",
    "                    if char == '\\\\':\n",
    "                        escape_next = True\n",
    "                        continue\n",
    "                        \n",
    "                    if char == '\"' and not escape_next:\n",
    "                        in_string = not in_string\n",
    "                        continue\n",
    "                    \n",
    "                    if not in_string:\n",
    "                        if char == '{':\n",
    "                            brace_depth += 1\n",
    "                        elif char == '}':\n",
    "                            brace_depth -= 1\n",
    "                        elif char == ',' and brace_depth == 1:\n",
    "                            # Found a top-level comma, indicating end of an entry\n",
    "                            count += 1\n",
    "                \n",
    "                # Show progress every million entries\n",
    "                if count > 0 and count % 1000000 == 0:\n",
    "                    print(f\"Processed ~{count/1000000:.1f}M entries so far...\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error counting entries: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Add 1 for the last entry (no comma after it)\n",
    "    if count > 0:\n",
    "        count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "# Count MedCorp entries\n",
    "medcorp_count = count_medcorp_entries()\n",
    "if medcorp_count:\n",
    "    print(f\"\\nMedCorp mapping file contains: {medcorp_count:,} entries\")\n",
    "    print(f\"MedCorp entries (millions): {medcorp_count / 1_000_000:.1f}M\")\n",
    "else:\n",
    "    print(\"Could not count MedCorp entries\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kare_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
