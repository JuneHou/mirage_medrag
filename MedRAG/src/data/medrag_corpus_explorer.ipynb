{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07e6b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09dd568",
   "metadata": {},
   "source": [
    "# MedRAG Corpus Statistics\n",
    "\n",
    "This notebook computes corpus statistics for the 4 MedRAG sources: PubMed, Wikipedia, Textbooks, and StatPearls.\n",
    "\n",
    "**Measurements:**\n",
    "- **#Doc.**: Number of unique documents \n",
    "- **#Snippets**: Number of text chunks\n",
    "- **Avg. L**: Average snippet length in characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20e06ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus directory: /data/wang/junh/githubs/mirage_medrag/MedRAG/src/data/corpus\n",
      "HF cache directory: /data/wang/junh/githubs/mirage_medrag/MedRAG/src/data/hf_cache\n",
      "Corpus exists: True\n",
      "HF cache exists: True\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "BASE_CORPUS_DIR = Path('/data/wang/junh/githubs/mirage_medrag/MedRAG/src/data/corpus')\n",
    "HF_CACHE_DIR = Path('/data/wang/junh/githubs/mirage_medrag/MedRAG/src/data/hf_cache')\n",
    "\n",
    "print('Corpus directory:', BASE_CORPUS_DIR)\n",
    "print('HF cache directory:', HF_CACHE_DIR)\n",
    "print('Corpus exists:', BASE_CORPUS_DIR.exists())\n",
    "print('HF cache exists:', HF_CACHE_DIR.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d389636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_content(obj):\n",
    "    \"\"\"Extract text content from a JSON object.\"\"\"\n",
    "    for field in ['content', 'contents', 'text', 'body']:\n",
    "        if field in obj and isinstance(obj[field], str):\n",
    "            return obj[field]\n",
    "    return \"\"\n",
    "\n",
    "def get_document_id(obj):\n",
    "    \"\"\"Extract document ID from a JSON object.\"\"\"\n",
    "    for field in ['PMID', 'pmid', 'paper_id', 'paperId', 'document_id', 'doc_id']:\n",
    "        if field in obj:\n",
    "            return str(obj[field])\n",
    "    return None\n",
    "\n",
    "def process_corpus(corpus_path):\n",
    "    \"\"\"Process a single corpus and return statistics.\"\"\"\n",
    "    corpus_name = corpus_path.name\n",
    "    chunk_dir = corpus_path / 'chunk'\n",
    "    \n",
    "    if not chunk_dir.exists():\n",
    "        print(f\"No chunk directory found for {corpus_name}\")\n",
    "        return None\n",
    "    \n",
    "    chunk_files = list(chunk_dir.glob('*.jsonl'))\n",
    "    if not chunk_files:\n",
    "        print(f\"No JSONL files found for {corpus_name}\")\n",
    "        return None\n",
    "    \n",
    "    total_snippets = 0\n",
    "    total_chars = 0\n",
    "    unique_docs = set()\n",
    "    \n",
    "    print(f\"Processing {corpus_name} ({len(chunk_files)} files)...\")\n",
    "    \n",
    "    for file_path in tqdm(chunk_files, desc=f\"{corpus_name}\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        total_snippets += 1\n",
    "                        \n",
    "                        # Get text content and length\n",
    "                        text = get_text_content(obj)\n",
    "                        total_chars += len(text)\n",
    "                        \n",
    "                        # Get document ID\n",
    "                        doc_id = get_document_id(obj)\n",
    "                        if doc_id:\n",
    "                            unique_docs.add(doc_id)\n",
    "                            \n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_length = total_chars / total_snippets if total_snippets > 0 else 0\n",
    "    n_docs = len(unique_docs) if unique_docs else total_snippets\n",
    "    \n",
    "    return {\n",
    "        'Corpus': corpus_name,\n",
    "        '#Doc.': n_docs,\n",
    "        '#Snippets': total_snippets,\n",
    "        'Avg. L': round(avg_length)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e07b76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä MedRAG Corpus Statistics (Correct Paths)\n",
      "==================================================\n",
      "üìÅ statpearls: 9625 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "statpearls: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9625/9625 [00:03<00:00, 2875.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ pubmed: 1166 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pubmed: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:01<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ wikipedia: 646 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wikipedia: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 646/646 [06:45<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ textbooks: 18 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textbooks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 18/18 [00:02<00:00,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä MEDRAG CORPUS STATISTICS\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>#Doc. (M)</th>\n",
       "      <th>#Snippets (M)</th>\n",
       "      <th>Avg. L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>statpearls</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pubmed</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>29.9</td>\n",
       "      <td>29.9</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>textbooks</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Corpus  #Doc. (M)  #Snippets (M)  Avg. L\n",
       "0  statpearls        0.4            0.4     516\n",
       "1      pubmed        0.1            0.1     915\n",
       "2   wikipedia       29.9           29.9     682\n",
       "3   textbooks        0.1            0.1     777"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ TOTAL: 30,524,583 docs, 30,524,583 snippets\n"
     ]
    }
   ],
   "source": [
    "# üéØ SIMPLIFIED CORRECT SOLUTION: Direct path processing\n",
    "print(\"üìä MedRAG Corpus Statistics (Correct Paths)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define correct paths for each corpus\n",
    "corpus_paths = {\n",
    "    'statpearls': BASE_CORPUS_DIR / 'statpearls' / 'chunk',\n",
    "    'pubmed': HF_CACHE_DIR / 'datasets--MedRAG--pubmed/snapshots' / list((HF_CACHE_DIR / 'datasets--MedRAG--pubmed/snapshots').glob('*'))[0].name / 'chunk',\n",
    "    'wikipedia': HF_CACHE_DIR / 'datasets--MedRAG--wikipedia/snapshots' / list((HF_CACHE_DIR / 'datasets--MedRAG--wikipedia/snapshots').glob('*'))[0].name / 'chunk',\n",
    "    'textbooks': HF_CACHE_DIR / 'datasets--MedRAG--textbooks/snapshots' / list((HF_CACHE_DIR / 'datasets--MedRAG--textbooks/snapshots').glob('*'))[0].name / 'chunk'\n",
    "}\n",
    "\n",
    "def process_corpus_simple(corpus_name, chunk_dir):\n",
    "    \"\"\"Simple corpus processing for correct statistics.\"\"\"\n",
    "    if not chunk_dir.exists():\n",
    "        print(f\"‚ùå {corpus_name}: Chunk directory not found\")\n",
    "        return None\n",
    "    \n",
    "    chunk_files = list(chunk_dir.glob('*.jsonl'))\n",
    "    if not chunk_files:\n",
    "        print(f\"‚ùå {corpus_name}: No JSONL files found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìÅ {corpus_name}: {len(chunk_files)} files\")\n",
    "    \n",
    "    total_snippets = 0\n",
    "    total_chars = 0\n",
    "    unique_docs = set()\n",
    "    \n",
    "    # NEW\n",
    "for file_path in tqdm(chunk_files, desc=f\"{corpus_name}\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        total_snippets += 1\n",
    "                        \n",
    "                        # Get text content\n",
    "                        text = get_text_content(obj)\n",
    "                        total_chars += len(text)\n",
    "                        \n",
    "                        # Get document ID  \n",
    "                        doc_id = get_document_id(obj)\n",
    "                        if doc_id:\n",
    "                            unique_docs.add(doc_id)\n",
    "                            \n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_length = total_chars / total_snippets if total_snippets > 0 else 0\n",
    "    n_docs = len(unique_docs) if unique_docs else total_snippets\n",
    "    \n",
    "    return {\n",
    "        'Corpus': corpus_name,\n",
    "        '#Doc.': n_docs,\n",
    "        '#Snippets': total_snippets,\n",
    "        'Avg. L': round(avg_length)\n",
    "    }\n",
    "\n",
    "# Process all corpora\n",
    "final_results = []\n",
    "for corpus_name, chunk_dir in corpus_paths.items():\n",
    "    stats = process_corpus_simple(corpus_name, chunk_dir)\n",
    "    if stats:\n",
    "        final_results.append(stats)\n",
    "\n",
    "# Display results\n",
    "if final_results:\n",
    "    df_final = pd.DataFrame(final_results)\n",
    "    \n",
    "    # Format numbers\n",
    "    df_final['#Doc. (M)'] = (df_final['#Doc.'] / 1_000_000).round(1) \n",
    "    df_final['#Snippets (M)'] = (df_final['#Snippets'] / 1_000_000).round(1)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä MEDRAG CORPUS STATISTICS\")\n",
    "    print(\"=\"*50)\n",
    "    display(df_final[['Corpus', '#Doc. (M)', '#Snippets (M)', 'Avg. L']])\n",
    "    \n",
    "    # Summary totals\n",
    "    total_docs = df_final['#Doc.'].sum()\n",
    "    total_snippets = df_final['#Snippets'].sum()\n",
    "    print(f\"\\nüéØ TOTAL: {total_docs:,} docs, {total_snippets:,} snippets\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data processed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
