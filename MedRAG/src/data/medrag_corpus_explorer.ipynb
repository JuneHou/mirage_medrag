{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07e6b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09dd568",
   "metadata": {},
   "source": [
    "# MedRAG Corpus Statistics\n",
    "\n",
    "This notebook computes corpus statistics for the 4 MedRAG sources: PubMed, Wikipedia, Textbooks, and StatPearls.\n",
    "\n",
    "**Measurements:**\n",
    "- **#Doc.**: Number of unique documents \n",
    "- **#Snippets**: Number of text chunks\n",
    "- **Avg. L**: Average snippet length in characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20e06ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus directory: /data/wang/junh/githubs/mirage_medrag/MedRAG/src/data/corpus\n",
      "HF cache directory: /data/wang/junh/githubs/mirage_medrag/MedRAG/src/data/hf_cache\n",
      "Corpus exists: True\n",
      "HF cache exists: True\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "BASE_CORPUS_DIR = Path('/data/wang/junh/githubs/mirage_medrag/MedRAG/src/data/corpus')\n",
    "HF_CACHE_DIR = Path('/data/wang/junh/githubs/mirage_medrag/MedRAG/src/data/hf_cache')\n",
    "\n",
    "print('Corpus directory:', BASE_CORPUS_DIR)\n",
    "print('HF cache directory:', HF_CACHE_DIR)\n",
    "print('Corpus exists:', BASE_CORPUS_DIR.exists())\n",
    "print('HF cache exists:', HF_CACHE_DIR.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d389636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_content(obj):\n",
    "    \"\"\"Extract text content from a JSON object.\"\"\"\n",
    "    for field in ['content', 'contents', 'text', 'body']:\n",
    "        if field in obj and isinstance(obj[field], str):\n",
    "            return obj[field]\n",
    "    return \"\"\n",
    "\n",
    "def get_document_id(obj):\n",
    "    \"\"\"Extract document ID from a JSON object.\"\"\"\n",
    "    for field in ['PMID', 'pmid', 'paper_id', 'paperId', 'document_id', 'doc_id']:\n",
    "        if field in obj:\n",
    "            return str(obj[field])\n",
    "    return None\n",
    "\n",
    "def process_corpus(corpus_path):\n",
    "    \"\"\"Process a single corpus and return statistics.\"\"\"\n",
    "    corpus_name = corpus_path.name\n",
    "    chunk_dir = corpus_path / 'chunk'\n",
    "    \n",
    "    if not chunk_dir.exists():\n",
    "        print(f\"No chunk directory found for {corpus_name}\")\n",
    "        return None\n",
    "    \n",
    "    chunk_files = list(chunk_dir.glob('*.jsonl'))\n",
    "    if not chunk_files:\n",
    "        print(f\"No JSONL files found for {corpus_name}\")\n",
    "        return None\n",
    "    \n",
    "    total_snippets = 0\n",
    "    total_chars = 0\n",
    "    unique_docs = set()\n",
    "    \n",
    "    print(f\"Processing {corpus_name} ({len(chunk_files)} files)...\")\n",
    "    \n",
    "    for file_path in tqdm(chunk_files, desc=f\"{corpus_name}\"):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        obj = json.loads(line)\n",
    "                        total_snippets += 1\n",
    "                        \n",
    "                        # Get text content and length\n",
    "                        text = get_text_content(obj)\n",
    "                        total_chars += len(text)\n",
    "                        \n",
    "                        # Get document ID\n",
    "                        doc_id = get_document_id(obj)\n",
    "                        if doc_id:\n",
    "                            unique_docs.add(doc_id)\n",
    "                            \n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_length = total_chars / total_snippets if total_snippets > 0 else 0\n",
    "    n_docs = len(unique_docs) if unique_docs else total_snippets\n",
    "    \n",
    "    return {\n",
    "        'Corpus': corpus_name,\n",
    "        '#Doc.': n_docs,\n",
    "        '#Snippets': total_snippets,\n",
    "        'Avg. L': round(avg_length)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de3e143f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pubmed (1166 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pubmed:   0%|          | 0/1166 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pubmed: 100%|██████████| 1166/1166 [03:57<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wikipedia (646 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wikipedia: 100%|██████████| 646/646 [03:04<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing textbooks (18 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textbooks: 100%|██████████| 18/18 [00:00<00:00, 22.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing statpearls (9625 files)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "statpearls: 100%|██████████| 9625/9625 [00:04<00:00, 2048.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MedRAG Corpus Statistics:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>#Doc. (M)</th>\n",
       "      <th>#Snippets (M)</th>\n",
       "      <th>Avg. L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pubmed</td>\n",
       "      <td>23.9</td>\n",
       "      <td>23.9</td>\n",
       "      <td>1309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>29.9</td>\n",
       "      <td>29.9</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>textbooks</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>statpearls</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Corpus  #Doc. (M)  #Snippets (M)  Avg. L\n",
       "0      pubmed       23.9           23.9    1309\n",
       "1   wikipedia       29.9           29.9     682\n",
       "2   textbooks        0.1            0.1     777\n",
       "3  statpearls        0.4            0.4     516"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw Numbers:\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>#Doc.</th>\n",
       "      <th>#Snippets</th>\n",
       "      <th>Avg. L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pubmed</td>\n",
       "      <td>23895135</td>\n",
       "      <td>23898701</td>\n",
       "      <td>1309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>29913202</td>\n",
       "      <td>29913202</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>textbooks</td>\n",
       "      <td>125847</td>\n",
       "      <td>125847</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>statpearls</td>\n",
       "      <td>352155</td>\n",
       "      <td>352155</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Corpus     #Doc.  #Snippets  Avg. L\n",
       "0      pubmed  23895135   23898701    1309\n",
       "1   wikipedia  29913202   29913202     682\n",
       "2   textbooks    125847     125847     777\n",
       "3  statpearls    352155     352155     516"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find available corpora\n",
    "corpus_names = ['pubmed', 'wikipedia', 'textbooks', 'statpearls']\n",
    "results = []\n",
    "\n",
    "for corpus_name in corpus_names:\n",
    "    corpus_path = BASE_CORPUS_DIR / corpus_name\n",
    "    if corpus_path.exists():\n",
    "        stats = process_corpus(corpus_path)\n",
    "        if stats:\n",
    "            results.append(stats)\n",
    "    else:\n",
    "        print(f\"Corpus not found: {corpus_path}\")\n",
    "\n",
    "# Create DataFrame\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Format numbers for better display\n",
    "    df['#Doc. (M)'] = (df['#Doc.'] / 1_000_000).round(1)\n",
    "    df['#Snippets (M)'] = (df['#Snippets'] / 1_000_000).round(1)\n",
    "    \n",
    "    # Add MedCorp total if available\n",
    "    if 'medcorp_count' in globals() and medcorp_count:\n",
    "        medcorp_row = {\n",
    "            'Corpus': 'MedCorp (Total)',\n",
    "            '#Doc.': medcorp_count,\n",
    "            '#Snippets': medcorp_count,\n",
    "            'Avg. L': '-',\n",
    "            '#Doc. (M)': round(medcorp_count / 1_000_000, 1),\n",
    "            '#Snippets (M)': round(medcorp_count / 1_000_000, 1)\n",
    "        }\n",
    "        df = pd.concat([df, pd.DataFrame([medcorp_row])], ignore_index=True)\n",
    "    \n",
    "    # Display the main table\n",
    "    display_df = df[['Corpus', '#Doc. (M)', '#Snippets (M)', 'Avg. L']].copy()\n",
    "    print(\"\\nMedRAG Corpus Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    display(display_df)\n",
    "    \n",
    "    # Also show raw numbers\n",
    "    print(\"\\nRaw Numbers:\")\n",
    "    print(\"=\" * 30)\n",
    "    display(df[['Corpus', '#Doc.', '#Snippets', 'Avg. L']])\n",
    "else:\n",
    "    print(\"No corpus data found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb9bbe",
   "metadata": {},
   "source": [
    "## Common Elements:\n",
    "1. **Required JSON keys**: id, title, content, contents\n",
    "2. **contents field**: concat(title, content) - title + \". \" + content\n",
    "3. **Whitespace normalization**: re.sub(\"\\s+\", \" \", text)\n",
    "4. **Helper functions**: ends_with_ending_punctuation(), concat()\n",
    "5. **Target chunk size**: ~1000 characters (with 200 overlap for long texts)\n",
    "6. **Output format**: JSONL (one JSON object per line)\n",
    "7. **Directory structure**: corpus/{source}/chunk/*.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39a33d",
   "metadata": {},
   "source": [
    "===============================================================================\n",
    "                    UMLS DATA SOURCE CREATION SUMMARY\n",
    "===============================================================================\n",
    "\n",
    "## ANALYSIS COMPLETED:\n",
    "\n",
    "### 1. Chunk Size Analysis from Existing Corpora:\n",
    "   - PubMed: 395-1897 chars (no chunking - abstracts are naturally short)\n",
    "   - Wikipedia: 559-900 chars (chunked with RecursiveCharacterTextSplitter)\n",
    "   - Textbooks: 935-970 chars (chunked with RecursiveCharacterTextSplitter)\n",
    "   - StatPearls: 533-634 chars (smart merging to keep <1000 chars)\n",
    "   \n",
    "   Target: ~1000 characters per chunk with 200 char overlap\n",
    "\n",
    "### 2. UMLS Communities Data Analysis:\n",
    "   - Total communities: 35,797\n",
    "   - Valid summaries: 34,620 (96.7%)\n",
    "   - Invalid (blank/error): 1,177 (3.3%)\n",
    "   - Summary length stats:\n",
    "     * Mean: 1,230 chars\n",
    "     * Median: 952 chars\n",
    "     * 95th percentile: 3,061 chars\n",
    "     * Max: 20,541 chars\n",
    "   - Summaries >1000 chars: 16,235 (46.9% of valid)\n",
    "   \n",
    "   Decision: YES, chunking is needed for long summaries\n",
    "\n",
    "### 3. General Data Processing Strategy (from 4 existing sources):\n",
    "   ✓ Common JSON structure: {id, title, content, contents}\n",
    "   ✓ contents = concat(title, content) with smart punctuation\n",
    "   ✓ Whitespace normalization: re.sub(\"\\s+\", \" \", text)\n",
    "   ✓ Target chunk size: 1000 chars with 200 overlap\n",
    "   ✓ Output format: JSONL (one JSON per line)\n",
    "   ✓ Directory: corpus/{source}/chunk/*.jsonl\n",
    "   ✓ Multiple files per source for organization\n",
    "\n",
    "### 4. UMLS Processing Strategy Implemented:\n",
    "   ✓ Group by run (35 runs total) - one JSONL file per run\n",
    "   ✓ Filter invalid summaries (errors, blank, \"too large\")\n",
    "   ✓ Hierarchical titles: \"UMLS -- Run X -- Level Y -- Community Z\"\n",
    "   ✓ Chunk long summaries (>1000 chars) with RecursiveCharacterTextSplitter\n",
    "   ✓ Keep short summaries as-is (like PubMed approach)\n",
    "   ✓ Unique IDs: \"UMLS_RX_LY_CZ\" or \"UMLS_RX_LY_CZ_chunkN\"\n",
    "\n",
    "## RESULTS:\n",
    "\n",
    "✓ Files created: 35 JSONL files (umls_run00.jsonl through umls_run34.jsonl)\n",
    "✓ Total size: 123 MB\n",
    "✓ Valid communities processed: 34,620\n",
    "✓ Invalid communities skipped: 1,177\n",
    "✓ Communities chunked: 16,235 (46.9%)\n",
    "✓ Total chunks created: 62,340\n",
    "\n",
    "Average chunks per file: 1,781\n",
    "Chunking rate: 46.9% (communities that needed splitting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
